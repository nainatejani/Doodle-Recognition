{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Doodle Recognition.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "106A67Drzr8NyUo9nqFi_s0MS4N293dxu",
      "authorship_tag": "ABX9TyMhq+K6zJdFNWwLdGd6aCkw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nainatejani/Doodle-Recognition/blob/master/Doodle_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kczijYEaIOHe",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yeCbrsLIqmU",
        "colab_type": "text"
      },
      "source": [
        "# Doodle Recognition\n",
        "\n",
        "My project is called Doodle Recognition. I use the [`Quick! Draw! Dataset`](https://colab.research.google.com/drive/106A67Drzr8NyUo9nqFi_s0MS4N293dxu#scrollTo=4yeCbrsLIqmU&line=3&uniqifier=1), which is the world's largest doodling dataset containing over 50 million drawings each belonging to one of the 345 categories. \n",
        "\n",
        "**Problem:** Recognize handmade drawings from a collection of 50 million drawings and place them in one of the 345 image categories, which include among other things, airplanes, books, body parts, etc. Analyse the accuracy of the model performance compared to human performance, and create a model that uses transfer learning from the doodles to recognize real objects.\n",
        "\n",
        "**The Importance of the Problem:** Important applications in computer vision and pattern recognition because doodles are high noise datasets.\n",
        "\n",
        "\n",
        "**Simplification of the Problem:**\n",
        "\n",
        "While ideally I would want to train my model for classification using a large amount of data, it is not very computationally feasible. For this reason, in this project, I restrict myself to a much smaller data sample. Moreover, I reduce the problem to a further extent by only considering 2 categories- ambulance and an angel. An ambulance is denoted by a 1 in my labels while 0 represents an angel. Given a doodle object which is either an ambulance or an angel, my classifier predicts which one it is likely to be.\n",
        "\n",
        "**Goals For The Project:** \n",
        "\n",
        "1. Classify doodles using the supervised ML algorithm called K-Nearest Neighbors. I analyze the accuracy and compare it with chance performace.\n",
        "2. Use a simple CNN to classify the doodles\n",
        "2. Use a pre-trained convolutional neural network to classify the doodles and report on the accuracy of the model\n",
        "3. Fine-tune the pretrained model on the doodle dataset and report on the accuracy of the model \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEojFEd4PPAK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# making necessary imports\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "# from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wp3qkdbOMOFh",
        "colab_type": "text"
      },
      "source": [
        "# Preparing the Dataset\n",
        "\n",
        "The `Quick! Draw! Dataset` can be downloaded in many formats e.g ndjson and numpy as well as raw versions. Since I am using pytorch and numpy, I found it best to use the numpy files.\n",
        "\n",
        "A note on the structure of the numpy files: each numpy file corresponds to all the doodles of one category. The entire numpy file is a list and each doodle is a 1D array in the list. As is clearly evitable, this calls for a lot of dataset preparation since there is no individual file for each datapoint. I prepare the dataset by using the following steps:\n",
        "1. Create a folder in the google drive containing 2 files for the 2 categories I am considering for my project. \n",
        "2. Using a python script, create a train_doodles_data and test_doodles_data as well as train_doodles_labels and test_doodles_labels.\n",
        "3. Use a class DoodleDataset inherited from the pytorch Dataset class to define train_load and test_load."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTQWUKytbJoK",
        "colab_type": "text"
      },
      "source": [
        "The following loads data from the folder \"logistic dataset\" from the drive. It loads the first 10000 doodles for each category as the train data and loads the doodles from index 15000 to 17000 for each category as the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfKjWRhLJp61",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# After some initial testing of the data, I found that each datapoint is a 1D array of size 784.\n",
        "doodles_train_data = np.array(np.zeros(784,))\n",
        "doodles_test_data = np.array(np.zeros(784))\n",
        "for root, dirs, files in os.walk(\"/content/drive/My Drive/logistic dataset\"):\n",
        "  for filename in files:\n",
        "    with open(\"/content/drive/My Drive/logistic dataset/\" + filename, \"rb\" ) as f:\n",
        "      print(\"First file to load is \" + str(filename))\n",
        "      data = np.load(f)\n",
        "      for i in range(10000):\n",
        "        doodles_train_data = np.vstack([doodles_train_data, data[i]])\n",
        "\n",
        "      for i in range(15000,17000):\n",
        "        doodles_test_data = np.vstack([doodles_test_data, data[i]])\n",
        "\n",
        " # Since I initialized the datasets with empty numpy array, I need to remove these \n",
        " # first np arrays from my dataset which I do by the following line of code.      \n",
        "doodles_train_data = doodles_train_data[1:,:]\n",
        "doodles_test_data = doodles_test_data[1:,:]\n",
        "\n",
        "print(doodles_train_data.shape)\n",
        "print(doodles_test_data.shape)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLmPATUES99W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# depending on how you want to label the classes. I am labelling 1 as ambulance(since I loaded in the ambulance dataset first\n",
        "#) and 0 as the angel.\n",
        "doodles_train_labels = np.concatenate((np.ones((10000,)), np.zeros((10000,))))\n",
        "doodles_test_labels = np.concatenate((np.ones((2000,)), np.zeros((2000,))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCPeGaQYXs-8",
        "colab_type": "text"
      },
      "source": [
        "# K-Nearest Neighbor Implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1P8L8pnVX2nt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " # I simplify the names for the KNN implementation.\n",
        " train_set_x, train_set_y, test_set_x, test_set_y= doodles_train_data, doodles_train_labels, doodles_test_data, doodles_test_labels\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onZlgfKwYFgs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(train_set_x)\n",
        "\n",
        "train_set_x = scaler.transform(train_set_x)\n",
        "test_set_x = scaler.transform(test_set_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhX_j85fYvnA",
        "colab_type": "code",
        "outputId": "5b791c82-2d64-4a09-dac0-4c01ebe31cdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "classifier = KNeighborsClassifier(n_neighbors=6)\n",
        "classifier.fit(train_set_x, train_set_y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
              "                     metric_params=None, n_jobs=None, n_neighbors=6, p=2,\n",
              "                     weights='uniform')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoNeqQw3Y1Ua",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = classifier.predict(test_set_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4E6fnvnbIGN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(confusion_matrix(test_set_y, y_pred))\n",
        "print(classification_report(test_set_y, y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MB8nyuD-cG4K",
        "colab_type": "text"
      },
      "source": [
        "The following are the results I received from implementing KNN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avufx2WHcpQE",
        "colab_type": "text"
      },
      "source": [
        "[[1913   87],\n",
        "[  67 1933]]\n",
        "\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "         0.0       0.97      0.96      0.96      2000\n",
        "         1.0       0.96      0.97      0.96      2000\n",
        "\n",
        "    accuracy                           0.96      4000\n",
        "    macro avg       0.96      0.96     0.96      4000\n",
        "    weighted avg    0.96      0.96     0.96      4000\n",
        "\n",
        "\n",
        "[Citation link text](https://stackabuse.com/k-nearest-neighbors-algorithm-in-python-and-scikit-learn/)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imtrUF5ubN4K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Note that this takes a long time to run and is optional. I ran this code and printed the error values in the following cell\n",
        "# so this can be skipped.\n",
        "error = []\n",
        "\n",
        "# Calculating error for K values between 1 and 40\n",
        "for i in range(1, 40):\n",
        "    print(i)\n",
        "    knn = KNeighborsClassifier(n_neighbors=i)\n",
        "    knn.fit(train_set_x, train_set_y)\n",
        "    pred_i = knn.predict(test_set_x)\n",
        "    error.append(np.mean(pred_i != test_set_y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1d6SGu6ud8i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "error = [0.0465, 0.04225, 0.0385, 0.03575, 0.0385, 0.035, 0.035, 0.036, 0.035, 0.0365, 0.0355, 0.0345, 0.0375, 0.03725, 0.0375, 0.0385, 0.03825, 0.0385, 0.03875]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aWi485qn3yC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(range(1, 20), error, color='red', linestyle='dashed', marker='o',\n",
        "         markerfacecolor='blue', markersize=10)\n",
        "plt.title('Error Rate K Value')\n",
        "plt.xlabel('K Value')\n",
        "plt.ylabel('Mean Error')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWzdGA5toswl",
        "colab_type": "text"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqo4XWNRpAoJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Transformation for image\n",
        "transform_ori = transforms.Compose([    #flipping the image horizontally\n",
        "                                    transforms.ToTensor(),                 #convert the image to a Tensor\n",
        "                                    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])])  #normalize the image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6KFCSL9Qcko",
        "colab_type": "code",
        "outputId": "f0031938-1dd3-46b5-cfc4-800b86c2231e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "class DoodleDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Face Landmarks dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, doodles, labels, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            csv_file (string): Path to the csv file with annotations.\n",
        "            root_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.doodles = doodles\n",
        "        self.labels = labels\n",
        "        self.transform = transform_ori \n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.doodles)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        sample = [self.doodles[idx], self.labels[idx]]\n",
        "        # {'doodle': self.doodles[idx], 'label': self.labels[idx]}\n",
        "\n",
        "        return sample\n",
        "\n",
        "train_dataset = DoodleDataset(doodles_train_data, doodles_train_labels,transforms.Compose([   \n",
        "                                    transforms.ToTensor(),                 #convert the image to a Tensor\n",
        "                                    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])]) )\n",
        "test_dataset = DoodleDataset(doodles_test_data,doodles_test_labels,transforms.Compose([   \n",
        "                                    transforms.ToTensor(),                 #convert the image to a Tensor\n",
        "                                    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])]))\n",
        "doodles_train_data = doodles_train_data.reshape((20000,1,28,28))\n",
        "doodles_test_data = doodles_test_data.reshape((4000,1,28,28))\n",
        "print(doodles_test_data.shape)\n",
        "print(doodles_train_data.shape)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4000, 1, 28, 28)\n",
            "(20000, 1, 28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1z9iE4nYUhAp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32\n",
        "train_load = torch.utils.data.DataLoader(dataset = train_dataset, \n",
        "                                         batch_size = batch_size,\n",
        "                                         shuffle = True)      #Shuffle to create a mixed batches of 100 of cat & dog images\n",
        "\n",
        "test_load = torch.utils.data.DataLoader(dataset = test_dataset, \n",
        "                                         batch_size = batch_size,\n",
        "                                         shuffle = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lX8WU6VWS5JA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualizing the data\n",
        "# Pass in any index to view the doodle. \n",
        "print(doodles_train_data.shape)\n",
        "image = doodles_train_data[150].reshape((28,28))\n",
        "plt.imshow(image, cmap = 'gray')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFiV8ZJ-pXq9",
        "colab_type": "text"
      },
      "source": [
        "# Basic CNN Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ev8y8ZI2pwlh",
        "colab_type": "text"
      },
      "source": [
        "The following code on basic CNN model is inspired by [this article.](https://nextjournal.com/gkoehler/pytorch-mnist)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drmIA4sR1der",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXATlA2fdNvp",
        "colab_type": "text"
      },
      "source": [
        "In the next cell, I define my CNN model using inbuilt nn module in Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhRHLnc2cwhJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(32, 32, kernel_size=5)\n",
        "        self.conv3 = nn.Conv2d(32,64, kernel_size=5)\n",
        "        self.fc1 = nn.Linear(3*3*64, 256)\n",
        "        self.fc2 = nn.Linear(256, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x.float()))\n",
        "        #x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = F.relu(F.max_pool2d(self.conv3(x),2))\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = x.view(-1,3*3*64 )\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        " \n",
        "cnn = CNN()\n",
        "print(cnn)\n",
        "\n",
        "it = iter(train_load)\n",
        "X_batch, y_batch = next(it)\n",
        "print(cnn.forward(X_batch).shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fW9dz2adUXT",
        "colab_type": "text"
      },
      "source": [
        "In the next cell, I define the fit for my model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKtjUPK0S5mu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "BATCH_SIZE = 32\n",
        "def fit(model, train_loader):\n",
        "    optimizer = torch.optim.Adam(model.parameters())#,lr=0.001, betas=(0.9,0.999))\n",
        "    error = nn.CrossEntropyLoss()\n",
        "    EPOCHS = 5\n",
        "    model.train()\n",
        "    for epoch in range(EPOCHS):\n",
        "        correct = 0\n",
        "        for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
        "            var_X_batch = Variable(X_batch).float()\n",
        "            var_y_batch = Variable(y_batch).long()\n",
        "            optimizer.zero_grad()\n",
        "            output = model(var_X_batch)\n",
        "            loss = error(output, var_y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Total correct predictions\n",
        "            predicted = torch.max(output.data, 1)[1] \n",
        "            correct += (predicted == var_y_batch).sum()\n",
        "            #print(correct)\n",
        "            if batch_idx % 50 == 0:\n",
        "              print(loss.data.shape)\n",
        "              loss.item()\n",
        "              print('Epoch : {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\t Accuracy:{:.3f}%'.format(\n",
        "                  epoch, batch_idx*len(X_batch), len(train_loader.dataset), 100.*batch_idx / len(train_loader), loss.item(), float(correct*100) / float(BATCH_SIZE*(batch_idx+1))))\n",
        "                "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hH5_9cfLU3QX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fit(cnn,train_load)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3lPTVmFdZhv",
        "colab_type": "text"
      },
      "source": [
        "I evaluate the model using the following code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRPV_W2lqkcS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model):\n",
        "#model = mlp\n",
        "    correct = 0 \n",
        "    for test_imgs, test_labels in test_load:\n",
        "        #print(test_imgs.shape)\n",
        "        test_imgs = Variable(test_imgs).float()\n",
        "        output = model(test_imgs)\n",
        "        predicted = torch.max(output,1)[1]\n",
        "        correct += (predicted == test_labels).sum()\n",
        "    print(\"Test accuracy:{:.3f}% \".format( float(correct) / (len(test_load)*BATCH_SIZE))*100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zTXVDOY8jAG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "evaluate(cnn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9733PKHztit",
        "colab_type": "text"
      },
      "source": [
        "Test set accuracy is 97%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mS_EEwnMpdfD",
        "colab_type": "text"
      },
      "source": [
        "# Using Pre-trained CNN and fine-tuning it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m14fLoEhrKlm",
        "colab_type": "text"
      },
      "source": [
        "The following code on pre-trained CNN using Resnet is inspired by a Pytoch [tutorial](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html) on transfer learning "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ilKRELHpqGM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doodles_train = doodles_train_data\n",
        "doodles_test = doodles_test_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56l4PZKDH_sZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import time\n",
        "import copy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCHuLq6ndi4W",
        "colab_type": "text"
      },
      "source": [
        "In the next 2 cells, I resize all my data in order to get 28,28,3 shaped doodles since the model Resnet18 only accepts 3 channels as inputs. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Awv1lZIiWg2p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This chunk might take a while to run.\n",
        "dim = np.zeros((28,28))\n",
        "doodles_train_resize = [np.array(np.zeros((3,28,28)))]\n",
        "i = 0\n",
        "for image in doodles_train_data:\n",
        "  print(i)\n",
        "  img = image.reshape((28,28))\n",
        "  resized = [np.stack((img,dim,dim), axis = 0)]\n",
        "  doodles_train_resize = np.vstack([doodles_train_resize, resized])\n",
        "  i+=1\n",
        "doodles_train_resize = np.array(doodles_train_resize)\n",
        "doodles_train_resize = doodles_train_resize[1:,:]\n",
        "print(doodles_train_resize.shape)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGWIIJ0Qp6tY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dim = np.zeros((28,28))\n",
        "doodles_test_resize = [np.array(np.zeros((3,28,28)))]\n",
        "# doodles_train_resize.shape\n",
        "i = 0\n",
        "for image in doodles_test_data:\n",
        "  print(i)\n",
        "  img = image.reshape((28,28))\n",
        "  resized = [np.stack((img,dim,dim), axis = 0)]\n",
        "  doodles_test_resize = np.vstack([doodles_test_resize, resized])\n",
        "\n",
        "  i+=1\n",
        "\n",
        "doodles_test_resize = np.array(doodles_test_resize)\n",
        "doodles_test_resize = doodles_test_resize[1:,:]\n",
        "print(doodles_test_resize.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DT1cj2x2d2zN",
        "colab_type": "text"
      },
      "source": [
        "I define my new Dataset Objects"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DvEm_rveaM1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset_resize = DoodleDataset(doodles_train_resize, doodles_train_labels)\n",
        "test_dataset_resize = DoodleDataset(doodles_test_resize, doodles_test_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljpxG9W4d8Et",
        "colab_type": "text"
      },
      "source": [
        "In the following chunk, i define dataloaders which will be used in my model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXVGHdX6fe4_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "datasets = {'train':train_dataset_resize, 'val':test_dataset_resize}\n",
        "dataloaders = {x: torch.utils.data.DataLoader(datasets[x], batch_size=32,\n",
        "                                             shuffle=True, num_workers=4)\n",
        "              for x in ['train', 'val']}\n",
        "dataset_sizes = {x: len(datasets[x]) for x in ['train', 'val']}\n",
        "# class_names = image_datasets['train'].classes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wG7MzenIeBv6",
        "colab_type": "text"
      },
      "source": [
        "The following code trains the model and also runs it on my test data to determine train as well as test accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7mbDBDoIB-c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs.float())\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels.long())\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k77fiuVXeLKw",
        "colab_type": "text"
      },
      "source": [
        "I define the model in the following chunk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiRoz1-TIRdj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_ft = models.resnet18(pretrained=True)\n",
        "num_ftrs = model_ft.fc.in_features\n",
        "# Here the size of each output sample is set to 2.\n",
        "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
        "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXehdVl3eOO2",
        "colab_type": "text"
      },
      "source": [
        "In the next cell, I run my model and get my results. i have copied the results just for reference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJsKY-vJFFdD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training and prediction both happens in the same line of code.\n",
        "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
        "                       num_epochs=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OWihqB6qOE-",
        "colab_type": "text"
      },
      "source": [
        "Epoch 0/4\n",
        "----------\n",
        "train Loss: 0.0381 Acc: 0.9858\n",
        "val Loss: 0.0491 Acc: 0.9790\n",
        "\n",
        "Epoch 1/4\n",
        "----------\n",
        "train Loss: 0.0243 Acc: 0.9900\n",
        "val Loss: 0.0535 Acc: 0.9800\n",
        "\n",
        "Epoch 2/4\n",
        "----------\n",
        "train Loss: 0.0197 Acc: 0.9926\n",
        "val Loss: 0.0515 Acc: 0.9788\n",
        "\n",
        "Epoch 3/4\n",
        "----------\n",
        "train Loss: 0.0144 Acc: 0.9942\n",
        "val Loss: 0.0683 Acc: 0.9768\n",
        "\n",
        "Epoch 4/4\n",
        "----------\n",
        "train Loss: 0.0138 Acc: 0.9946\n",
        "val Loss: 0.0588 Acc: 0.9803\n",
        "\n",
        "Training complete in 1m 57s\n",
        "Best val Acc: 0.980250"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWTWMoTNq1ik",
        "colab_type": "text"
      },
      "source": [
        "# Using Pre-Trained CNN without fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKLyDUMbIhhT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_conv = torchvision.models.resnet18(pretrained=True)\n",
        "for param in model_conv.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Parameters of newly constructed modules have requires_grad=True by default\n",
        "num_ftrs = model_conv.fc.in_features\n",
        "model_conv.fc = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "model_conv = model_conv.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that only parameters of final layer are being optimized as\n",
        "# opposed to before.\n",
        "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJKjCmXSqsm7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_conv = train_model(model_conv, criterion, optimizer_conv,\n",
        "                         exp_lr_scheduler, num_epochs=25)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzqcxolHry4C",
        "colab_type": "text"
      },
      "source": [
        "Epoch 0/24\n",
        "----------\n",
        "train Loss: 0.3882 Acc: 0.8272\n",
        "val Loss: 0.3354 Acc: 0.8580\n",
        "\n",
        "Epoch 1/24\n",
        "----------\n",
        "train Loss: 0.3561 Acc: 0.8457\n",
        "val Loss: 0.3587 Acc: 0.8465\n",
        "\n",
        "Epoch 2/24\n",
        "----------\n",
        "train Loss: 0.3508 Acc: 0.8502\n",
        "val Loss: 0.3121 Acc: 0.8700\n",
        "\n",
        "Epoch 3/24\n",
        "----------\n",
        "train Loss: 0.3424 Acc: 0.8526\n",
        "val Loss: 0.3223 Acc: 0.8742\n",
        "\n",
        "Epoch 4/24\n",
        "----------\n",
        "train Loss: 0.3471 Acc: 0.8527\n",
        "val Loss: 0.3347 Acc: 0.8650\n",
        "\n",
        "Epoch 5/24\n",
        "----------\n",
        "train Loss: 0.3461 Acc: 0.8529\n",
        "val Loss: 0.3377 Acc: 0.8563\n",
        "\n",
        "Epoch 6/24\n",
        "----------\n",
        "train Loss: 0.3408 Acc: 0.8563\n",
        "val Loss: 0.3262 Acc: 0.8708\n",
        "\n",
        "Epoch 7/24\n",
        "----------\n",
        "train Loss: 0.3300 Acc: 0.8609\n",
        "val Loss: 0.3215 Acc: 0.8685\n",
        "\n",
        "Epoch 8/24\n",
        "----------\n",
        "train Loss: 0.3184 Acc: 0.8655\n",
        "val Loss: 0.3070 Acc: 0.8788\n",
        "\n",
        "Epoch 9/24\n",
        "----------\n",
        "train Loss: 0.3198 Acc: 0.8663\n",
        "val Loss: 0.3141 Acc: 0.8705\n",
        "\n",
        "Epoch 10/24\n",
        "----------\n",
        "train Loss: 0.3197 Acc: 0.8651\n",
        "val Loss: 0.3252 Acc: 0.8702\n",
        "\n",
        "Epoch 11/24\n",
        "----------\n",
        "train Loss: 0.3238 Acc: 0.8634\n",
        "val Loss: 0.3147 Acc: 0.8730\n",
        "\n",
        "Epoch 12/24\n",
        "----------\n",
        "train Loss: 0.3204 Acc: 0.8659\n",
        "val Loss: 0.3168 Acc: 0.8708\n",
        "\n",
        "Epoch 13/24\n",
        "----------\n",
        "train Loss: 0.3188 Acc: 0.8620\n",
        "val Loss: 0.3085 Acc: 0.8732\n",
        "\n",
        "Epoch 14/24\n",
        "----------\n",
        "train Loss: 0.3188 Acc: 0.8651\n",
        "val Loss: 0.3150 Acc: 0.8678\n",
        "\n",
        "Epoch 15/24\n",
        "----------\n",
        "train Loss: 0.3152 Acc: 0.8659\n",
        "val Loss: 0.3141 Acc: 0.8668\n",
        "\n",
        "Epoch 16/24\n",
        "----------\n",
        "train Loss: 0.3188 Acc: 0.8649\n",
        "val Loss: 0.3190 Acc: 0.8710\n",
        "\n",
        "Epoch 17/24\n",
        "----------\n",
        "train Loss: 0.3201 Acc: 0.8649\n",
        "val Loss: 0.3140 Acc: 0.8720\n",
        "\n",
        "Epoch 18/24\n",
        "----------\n",
        "train Loss: 0.3163 Acc: 0.8631\n",
        "val Loss: 0.3168 Acc: 0.8715\n",
        "\n",
        "Epoch 19/24\n",
        "----------\n",
        "train Loss: 0.3163 Acc: 0.8663\n",
        "val Loss: 0.3060 Acc: 0.8775\n",
        "\n",
        "Epoch 20/24\n",
        "----------\n",
        "train Loss: 0.3186 Acc: 0.8676\n",
        "val Loss: 0.3068 Acc: 0.8765\n",
        "\n",
        "Epoch 21/24\n",
        "----------\n",
        "train Loss: 0.3138 Acc: 0.8665\n",
        "val Loss: 0.3089 Acc: 0.8700\n",
        "\n",
        "Epoch 22/24\n",
        "----------\n",
        "train Loss: 0.3173 Acc: 0.8653\n",
        "val Loss: 0.3083 Acc: 0.8690\n",
        "\n",
        "Epoch 23/24\n",
        "----------\n",
        "train Loss: 0.3142 Acc: 0.8659\n",
        "val Loss: 0.3160 Acc: 0.8702\n",
        "\n",
        "Epoch 24/24\n",
        "----------\n",
        "train Loss: 0.3116 Acc: 0.8692\n",
        "val Loss: 0.3136 Acc: 0.8748\n",
        "\n",
        "Training complete in 3m 20s\n",
        "Best val Acc: 0.878750"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmW9F3i8qtbq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}